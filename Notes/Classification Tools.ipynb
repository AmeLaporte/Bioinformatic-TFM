{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SINTAX: a simple non-Bayesian taxonomy classifier for 16S and ITS sequences_Robert C. Edgar\n",
        "\n",
        "test with the leave one out method but it is considered not as an accurate estimation of the efficency of the tool.\n",
        "\n",
        "Clade partition cross validation (CPX)\n",
        "\n",
        "k-mer based\n",
        "\n",
        "Does not require training.\n",
        "\n",
        "Metrics calculation:\n",
        "\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sensibility= TP/N known\n",
        "overclassification rate: OC= FP over/ N novel\n",
        "misclassification: MC= FP mis/ N known\n",
        "Errors per query: EPQ= (FPmis + FPover)/N"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SINTAX algorithm predicts taxonomy by using k-mer similarity to identify top-hit in a reference database and provide bootstrap confidence for all ranks in the prediction. Does not require training.\n",
        "\nMost tested methods are shown to have high rates of over-classification errors where novel taxa are incorrectly predicted to have known names."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "UTAX: \n",
        "\n",
        "No leave one out test, but kind of a more stric clade partition cross-validation: test the errors on different levels if the level studied is totally taken off the database.\n",
        "\n",
        "reliability score.\n",
        "\n",
        "Requires dataset training.\n",
        "\n",
        "(from the website)\n",
        "\n",
        " However, it is possible to measure the relative performance of different classification algorithms by dividing a gold standard reference set into training and test sets and using the training set as a reference for the stand-alone programs. For my benchmark tests, I used the RDP 16S training data and the UNITE database as trusted references.\n",
        " \n",
        " The RDP \"leave-one-out\" method\n",
        "The RDP classifier papers use leave-one-out. which works as follows. They take their training database (D) and remove one sequence (call it Q, the query sequence) leaving a reduced database D'. They re-train their classifier using D' and test whether Q is correctly classified. They repeat this for all sequences in the database. Each level is tested (genus, family...) to measure \"accuracy\", defined as the percentage of correctly classified sequences.\n",
        "\n",
        "The RDP validation method grossly over-states accuracy\n",
        "The RDP leave-one-out method is highly misleading.  This reasonable only if you believe that most genera in your sample are present in your reference set. It does not test how well the classifier performs when challenged with novel taxa at any level.\n",
        " Accuracy at each rank is defined to be the fraction of queries for which the taxon at that rank is correctly identified. With RDP14 as the reference, the probability that the genus of a random query is present after removing the query is 91% (far greater than the most optimistic estimate of 0.2% for a random species) with a mean of 4.2 remaining training sequences for its genus, and 99.5% that the family is present with a mean of 27 training sequences. Leave-one-out thus models a highly unrealistic scenario where the reference database has several training examples for all ranks of most query sequences.\n",
        "\n\n",
        "So this is my validation protocol: For each taxonomic level (genus, family... phylum), make two Query-Database pairs by splitting your trusted reference set. In one pair (the \"possible\" pair), at least one example is present in the database so the classification is possible. This measures sensitivity and the error rate of misclassifying to a different taxon. The second Q-D pair (\"impossible\") has no examples of the taxon, so assignments at the given level are always errors. This measures the rate of overclassification errors.\n",
        "UTAX gives an estimated probability that the prediction is correct for each level (genus, family...).\n",
        "\n\n",
        " Lowest Common Rank (LCR)\n",
        "The lowest common rank is defined to be the lowest level (genus, family etc.) that is present in the reference set (training set). For example, if the genus is not present but the family is present, then the lowest common level is family. Predicting the LCR is the most difficult challenge for a taxonomy classification algorithm. For example, if the identity of the top hit is 88%, should be LCR be class, family, genus or what?\n",
        "\n",
        " Overclassification error\n",
        "If a classifier predicts a level lower than the LCR, then this is an overclassification error. For example, the lowest common level is family but a genus is predicted.\n",
        "\n",
        "Underclassification error\n",
        "If a classifier predicts higher levels but does not predict the LCR, then this is an underclassification error. For example, the lowest common level is family but the lowest predicted level is order. All false negatives are underclassification errors so there is really no need for a new term.\n",
        " \n",
        " \n",
        "(1) reference databases are missing most species, and (2) taxonomies were mostly assigned before molecular sequence data was available, so there are conflicts and inconsistencies between sequence and taxonomy.\n",
        "\n",
        "The definition and interpretation of a taxonomy prediction confidence estimate is not as simple as it might appear. Ideally, the error rate of predictions with confidence 0.9 should be approximately 10%, but in practice the error rate depends on the query dataset and on unknown characteristics of the reference dataset. It would be nice to calculate a p-value, but this is tricky because we need two statistical models: one for the hypothesis we are testing plus a null model in which the hypothesis is false and the observation occurs by chance. I don't have a clue how to do this for taxonomy predictions.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "METAXA2: improved identification and taxonomic classification of small and large subunit rRNA in metagenomic data _ J. Bengtsson-Palme, M. Hartmann, K.M Eriksson et al.\n",
        "\n",
        "reliability score\n",
        "\n",
        "Database: curated from SILVA 111 and MITOZOA. ( no sequences without complete taxonomic affiliation to the species level, no sequences known to be chimeric, no sequence from uncultured/metagenomic project, no sequences with contradictory taxonomic information)\n",
        "\n",
        "Classification possible for archea, bacteria, nucleus of eukaryotes, mitochondria, chloroplast. (classification beyong organelle type is not possible)\n",
        "\nTest with the curated dataset and also with different whole database (non redundant SILVA 111 and Greengenes 13-8)."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "node_nteract",
      "language": "javascript",
      "display_name": "Node.js (nteract)"
    },
    "kernel_info": {
      "name": "node_nteract"
    },
    "language_info": {
      "name": "javascript",
      "version": "6.5.0",
      "mimetype": "application/javascript",
      "file_extension": ".js"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
